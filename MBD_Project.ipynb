{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_Lv2jO0cvcSJ",
        "RFhNmF_Gwh3E",
        "Pr8nfduAI4Ex",
        "nZjAIevZH4Sr",
        "Ys9xCPgiGsun",
        "rC2OjodEv6dI"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initializations"
      ],
      "metadata": {
        "id": "_Lv2jO0cvcSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "# Timeout in seconds for an observation run. \n",
        "# The longer the time period, the more accurate the probabilities and the diagnoses. \n",
        "# The resulting text file shows a status per observation if the program could succeed in completing the process or not.\n",
        "TIMEOUT_PER_OBSERVATION     = 30*60\n",
        "\n",
        "USE_GOOGLE_DRIVE_FOR_FILES    = True\n",
        "\n",
        "if USE_GOOGLE_DRIVE_FOR_FILES:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  DATA_FOLDER_PATH              = \"/content/drive/My Drive/Data Science/BGU/Anomaly Detection and Diagnosis/Project/Program Files/\"\n",
        "\n",
        "else:\n",
        "  DATA_FOLDER_PATH = \"\""
      ],
      "metadata": {
        "id": "iWhP3idvvgwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1f87de-e49a-4dbd-f4e7-bd53ab05be48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gates"
      ],
      "metadata": {
        "id": "RFhNmF_Gwh3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Gate:\n",
        "  \"\"\"\n",
        "  A logical gate in the system\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  action:                  str,\n",
        "                           the gate type. supported types:  'and', 'buffer', 'inverter', 'nand', 'nor', 'or', 'xor'.\n",
        "  gate_size:               int,\n",
        "                           the number of inputs to the gate.\n",
        "  name:                    str,\n",
        "                           the name of the gate as it appears in the file.\n",
        "  input_nicknames:         str,\n",
        "                           the input names as they appear in the file.\n",
        "  output_nickname:         str,\n",
        "                           the output name as it appears in the file.                         \n",
        "  \"\"\"\n",
        "  def __init__(self, action, gate_size, name, input_nicknames, output_nickname, p_gate):\n",
        "    self.__action = action\n",
        "    self.__gate_size = gate_size\n",
        "    self.__name = name\n",
        "    self.__input_nicknames = input_nicknames\n",
        "    self.__output_nickname = output_nickname\n",
        "    self.__p_gate_faulty = p_gate\n",
        "    self.__input_indexes = []\n",
        "    self.__output_index = None\n",
        "    self.__rank = None\n",
        "    self.__processor = {}\n",
        "    self.__faulty = False\n",
        "    self.initialize_dictionary()\n",
        "   \n",
        "\n",
        "\n",
        "  def initialize_io(self, signals_names):\n",
        "    \"\"\"\n",
        "    initalizes the input and output indexes of the given signals_names.\n",
        "    e.g: for c17, i1,i3 will be initailized as [0,2]\n",
        "    \"\"\"\n",
        "    for name in self.__input_nicknames:\n",
        "      self.__input_indexes.append(signals_names.index(name))\n",
        "    self.__output_index= signals_names.index(self.__output_nickname)\n",
        "    \n",
        "\n",
        "\n",
        "  def initialize_dictionary(self):\n",
        "    \"\"\"\n",
        "    creates a dictionary of all possible values for the gate, for fast processing during runtime.\n",
        "    \"\"\"\n",
        "    found = True\n",
        "    dict_values = initialize_permutations(self.__gate_size)\n",
        "    \n",
        "    for dict_key_values in dict_values:\n",
        "      dict_key_values_str = [str(i) for i in dict_key_values]\n",
        "      final_key = \"\".join(dict_key_values_str)\n",
        "      if self.__action == \"and\":\n",
        "        result = 0 if '0' in final_key else 1\n",
        "      elif self.__action == \"buffer\":\n",
        "        result = 0 if final_key=='0' else 1\n",
        "      elif self.__action == \"inverter\":\n",
        "        result = 0 if final_key=='1' else 1\n",
        "      elif self.__action == \"nand\":\n",
        "        result = 1 if '0' in final_key else 0\n",
        "      elif self.__action == \"nor\":\n",
        "        result = 0 if '1' in final_key else 1\n",
        "      elif self.__action == \"or\":\n",
        "        result = 1 if '1' in final_key else 0\n",
        "      elif self.__action == \"xor\":\n",
        "        if self.__gate_size > 2:\n",
        "          raise ValueError(f\"missing initialization for {self.__action}{self.__gate_size}\")\n",
        "        result = 0 if final_key in ['00', '11'] else 1\n",
        "      else:\n",
        "        raise ValueError(f\"missing initialization for {self.__action}\")\n",
        "      \n",
        "      final_key = '[' + \", \".join(final_key) + ']'\n",
        "      self.__processor[final_key] = result\n",
        "\n",
        "\n",
        "\n",
        "  def process(self, inputs_arr):\n",
        "    \"\"\"\n",
        "    the method which is called during runtime, to calculate the gate's output.\n",
        "    inputs_arr - an array with the inputs signals.\n",
        "    \"\"\"\n",
        "    ret = self.__processor[str(inputs_arr)]\n",
        "    return ret ^ (1 if self.__faulty else 0)\n",
        "\n",
        "\n",
        "\n",
        "  def __str__(self):\n",
        "    \"\"\"\n",
        "    prints the gate properties.\n",
        "    \"\"\"\n",
        "    print(f\"action:\\t\\t {self.__action}\")\n",
        "    print(f\"gate_size:\\t {self.__gate_size}\")\n",
        "    print(f\"name:\\t\\t {self.__name}\")\n",
        "    print(f\"input_nicknames: {self.__input_nicknames}\")\n",
        "    print(f\"output_nicknames: {self.__output_nickname}\")\n",
        "    print(f\"input_indexes:\\t {self.__input_indexes}\")\n",
        "    print(f\"output_index:\\t {self.__output_index}\")\n",
        "    print(f\"rank:\\t\\t {self.__rank}\")\n",
        "    return \"\"\n",
        "\n",
        "  @property\n",
        "  def input_nicknames(self):\n",
        "    return self.__input_nicknames\n",
        "\n",
        "  @property\n",
        "  def output_nickname(self):\n",
        "    return self.__output_nickname\n",
        "\n",
        "  @property\n",
        "  def input_indexes(self):\n",
        "    return self.__input_indexes\n",
        "\n",
        "  @property\n",
        "  def output_index(self):\n",
        "    return self.__output_index\n",
        "\n",
        "  @property\n",
        "  def rank(self):\n",
        "    return self.__rank\n",
        "\n",
        "  @property\n",
        "  def p_gate_faulty(self):\n",
        "    return self.__p_gate_faulty\n",
        "\n",
        "  @rank.setter\n",
        "  def rank(self, rank):\n",
        "    self.__rank = rank\n",
        "\n",
        "  @property\n",
        "  def name(self):\n",
        "    return self.__name\n",
        "\n",
        "  @property\n",
        "  def faulty(self):\n",
        "    return self.__faulty\n",
        "\n",
        "  @faulty.setter\n",
        "  def faulty(self, faulty):\n",
        "    self.__faulty = faulty\n",
        "\n",
        " \n",
        "\n",
        "def initialize_gates(system:str, p_gate):\n",
        "  \"\"\"\n",
        "    reads the system file and initalizes the gates and the signals vector.\n",
        "\n",
        "    p_gate      - the probability of the gate to be faulty.\n",
        "\n",
        "\n",
        "    Returns:\n",
        "    gates       - list of Gate object representing the system gates\n",
        "    signals     - list of the system signals (in/out), with elements initialzied as None. \n",
        "    inputs_num  - the number of inputs in the signals list, starting from index 0.\n",
        "    outputs_num - the number of outputs in the signals list, starting from index inputs_num.\n",
        "    \n",
        "  \"\"\"\n",
        "\n",
        "  gates=[]\n",
        "  signals = []\n",
        "  signals_names = []\n",
        "\n",
        "  filename_sys = DATA_FOLDER_PATH + system + '.sys'\n",
        "\n",
        "  with open(filename_sys) as f:\n",
        "    content = f.read()\n",
        "\n",
        "  content_list = (content.replace(\"\\n\",\"\")).split(\".\")\n",
        "\n",
        "  ###############################################################################################\n",
        "  # load gates\n",
        "  ###############################################################################################\n",
        "  items = content_list[3].split('],[')\n",
        "\n",
        "  for gate in items:\n",
        "    \n",
        "    l = (gate.replace(\"[\",\"\").replace(\"]\",\"\")).split(',')\n",
        "\n",
        "    # extract action and inputs_len (e.g: 'nand2')\n",
        "    gate_logic_type = l[0]\n",
        "    \n",
        "    gate_action = re.findall(\"[a-z]+\",gate_logic_type)[0]\n",
        "    if gate_action not in ('and', 'buffer', 'inverter', 'nand', 'nor', 'or', 'xor'):\n",
        "      raise ValueError(\"Missing gate validation: \",gate_action)\n",
        "\n",
        "    if len(gate_action) != len(gate_logic_type):\n",
        "      # there's a number\n",
        "      gate_inputs_len = int(re.findall(\"[0-9]+\",gate_logic_type)[0])\n",
        "    else:\n",
        "      gate_inputs_len = 1\n",
        "\n",
        "    gate_inputs_nickname = []  \n",
        "    for i in range(gate_inputs_len):\n",
        "      gate_inputs_nickname.append(l[3+i])\n",
        "\n",
        "    # Gate object creation\n",
        "    gates.append(Gate(action=gate_action, gate_size=gate_inputs_len, name=l[1], input_nicknames=gate_inputs_nickname, output_nickname=l[2], p_gate=p_gate))\n",
        "\n",
        "\n",
        "  ###############################################################################################\n",
        "  # initialize the signals array with inputs, outputs and z-values\n",
        "  ###############################################################################################\n",
        "  # inputs (e.g: i1,i3)\n",
        "  items = (content_list[1].replace('[',\"\").replace(']',\"\")).split(',')\n",
        "  inputs_num = len(items)\n",
        "  for i in items:\n",
        "    signals_names.append(i)\n",
        "    signals.append(None)\n",
        "\n",
        "  # outputs (e.g: o1)\n",
        "  items = (content_list[2].replace('[',\"\").replace(']',\"\")).split(',')\n",
        "  outputs_num = len(items)\n",
        "  for i in items:\n",
        "    signals_names.append(i)\n",
        "    signals.append(None)\n",
        "\n",
        "  # z values (intermediate gates, e.g: z4,z2)\n",
        "  for gate in gates:\n",
        "    # what's missing are the gate's inputs and outputs which are not already in signals\n",
        "    missing = ( set(gate.input_nicknames) | {gate.output_nickname} ) - set(signals_names)\n",
        "    for item in missing:\n",
        "      signals_names.append(item)\n",
        "      signals.append(None)\n",
        "\n",
        "  ###############################################################################################\n",
        "  # initailize the gates inputs to their indexes in the signal array\n",
        "  ###############################################################################################\n",
        "  for gate in gates:\n",
        "    gate.initialize_io(signals_names)\n",
        "\n",
        "  ###############################################################################################\n",
        "  # rank the gates\n",
        "  ###############################################################################################\n",
        "\n",
        "  # simulate 0 inputs\n",
        "  for i in range(inputs_num):\n",
        "    signals[i] = 0\n",
        "\n",
        "  rank=0\n",
        "  while None in signals:\n",
        "\n",
        "    for gate in gates: \n",
        "      \n",
        "      # retrieve the gate's signals according to its input indexes\n",
        "      in_signals = [signals[i] for i in gate.input_indexes]\n",
        "\n",
        "      # all signals arrived\n",
        "      if None not in in_signals and gate.rank == None:\n",
        "        gate.rank = rank\n",
        "        \n",
        "    # set the output for all the gates in this rank\n",
        "    for gate in gates:\n",
        "      if gate.rank == rank:\n",
        "        signals[gate.output_index] = 0 # simulate 0 output\n",
        "    \n",
        "    rank+=1\n",
        "    \n",
        "  # reset\n",
        "  for i in range(len(signals)):\n",
        "    signals[i] = None\n",
        "\n",
        "  return gates, signals, inputs_num, outputs_num\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def initialize_observations(system:str, gates, signals_len, inputs_num, outputs_num):\n",
        "  \"\"\"\n",
        "  \n",
        "  reads the observations file and returns the io vector.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  filename_dat = DATA_FOLDER_PATH + system + '_iscas85.obs'\n",
        "  with open(filename_dat) as f:\n",
        "    content = f.read()\n",
        "\n",
        "  content_list = (content.replace(\"\\n\",\"\")).split(\".\") # separate the lines\n",
        "  if content_list[-1] == \"\":\n",
        "    content_list = content_list[:-1]\n",
        "  content_list = [item[item.find('[')+1:item.find(']')] for item in content_list] # isolate the io vector \n",
        "  content_list = [item.split(\",\") for item in content_list] # create a list from the textual vector\n",
        "\n",
        "  # initialize the io vectors\n",
        "  io_vectors = [[0 if i[0]=='-' else 1 for i in item] for item in content_list] # present in binary 0/1\n",
        "\n",
        "  return io_vectors\n",
        "\n",
        "\n",
        "\n",
        "def initialize_noise(outputs_num, print_result=False):\n",
        "  \"\"\"\n",
        "  \n",
        "  initializes random noise for the system's outputs.\n",
        "\n",
        "  \"\"\"\n",
        "  ret = [0.01 * random.randint(1,10) for i in range(outputs_num)]\n",
        "  if print_result:\n",
        "    for i,p in enumerate(ret):\n",
        "      print(f\"Out{i}={p}, \", end=\"\")\n",
        "    print(\"\\n\")\n",
        "  return ret\n",
        "\n",
        "          \n",
        "\n",
        "def initialize_permutations(outputs_num):\n",
        "  \"\"\"\n",
        "  \n",
        "  returns a list with all possible outputs combintation (e.g: [0,0], [0,1], [1,0], [1,1]).\n",
        "\n",
        "  \"\"\"\n",
        "  res = []\n",
        "  temp = []\n",
        "  for i in range(pow(2,outputs_num)):\n",
        "    s=(\"0\"*outputs_num + bin(i).replace(\"0b\", \"\"))[-outputs_num:]\n",
        "    temp.append(list(s))\n",
        "  for item in temp:\n",
        "    res.append([int(i) for i in item])\n",
        "  return res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def initialize_ranks(gates):\n",
        "  \"\"\"\n",
        "  \n",
        "  returns a list of lists. \n",
        "  The external list i-th index represent the i-th rank in the system.\n",
        "  The internal list contains the gate index in the gates list.\n",
        "\n",
        "  \"\"\"\n",
        "  gate_rank_indexes = []\n",
        "  max_rank = max([i.rank for i in gates])\n",
        "  \n",
        "  for i_rank, rank in enumerate(range(max_rank+1)):\n",
        "    gate_rank_indexes.append([])\n",
        "    for i_gate, gate in enumerate(gates):\n",
        "      if gate.rank == rank:\n",
        "        gate_rank_indexes[i_rank].append(i_gate)\n",
        "      \n",
        "  return gate_rank_indexes, max_rank\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_gates_sequence(gates_queue_method, gates, gate_rank_indexes):\n",
        "  \"\"\"\n",
        "  \n",
        "  returns a list which is used as a queue of nodes for the dfs method.\n",
        "  the list represents the sequence in which the gates should be processed during the tree creation.\n",
        "  gates_queue_method - 'SERIAL' - according to the order in the ISCAS-85 file. Left most node is 0.\n",
        "                       'RANKED' - considers the model gate ranks. The higher the proximity to the outputs, the higher the rank. \n",
        "                                  high ranked gates are accessed first. \n",
        "\n",
        "  \"\"\"\n",
        "  if gates_queue_method == 'RANKED':\n",
        "      return [subitem for item in gate_rank_indexes[::-1] for subitem in item]\n",
        "  elif gates_queue_method == 'SERIAL':\n",
        "    return list(np.arange(len(gates)))\n",
        "  raise ValueError('Unknown queue method: ', gates_queue_method)\n"
      ],
      "metadata": {
        "id": "oF_-0B18qXh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree Node"
      ],
      "metadata": {
        "id": "Pr8nfduAI4Ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Node:\n",
        "  \"\"\"\n",
        "  A DFS tree node\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  path:                    list of gate indexes\n",
        "  branch_out_success:      list in the length of 2^outputs. The i-th index represent the status of the i-th output permutation. \n",
        "                           statuses: 0 = diagnoses not found yet for this output\n",
        "                                     1 = diagnoses for this output found in this iteration\n",
        "                                     2 = diagnoses for this output found in previous iterations\n",
        "  n_gates:                 int, \n",
        "                           the number of gates in the system\n",
        "  n_outputs:               int, \n",
        "                           the number of outputs in the system\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, path, branch_out_success, n_gates, n_outputs, print_interval=1_000_000):\n",
        "    global run_counter\n",
        "    global run_timer\n",
        "    run_counter+=1\n",
        "    \n",
        "    self.__path = path\n",
        "\n",
        "    self.__n_gates = n_gates \n",
        "    self.__n_outputs = n_outputs   \n",
        "    self.__nodes = []\n",
        "    self.__out_success = [2 if item==1 else item for item in branch_out_success]\n",
        "\n",
        "    if run_counter%print_interval==0:\n",
        "      print(f\"Node {run_counter}, {datetime.now()-run_timer}, {path}\")\n",
        "      run_timer = datetime.now()\n",
        "\n",
        "\n",
        "\n",
        "  def dfs(self, full_path, prune_level, pre_diagnosis, pre_diagnosis_size, gates, gate_rank_indexes, inputs_len, outputs_len, signals, gates_input_indexes, timeout_per_observation, initial_time):\n",
        "    \"\"\"\n",
        "    \n",
        "    A recursive method. Evaluates for the current path and stores successful statuses (diagnoses found).\n",
        "    \n",
        "    full_path               - a complete list of the gates sequence in the tree.\n",
        "    prune_level             - int. if bigger than -1, the tree will return upon reaching the prune_level depth.\n",
        "    pre_diagnosis           - a list of diagnoses per output. \n",
        "                              when supplied, the method will look in it to find if it includes the recent pre_diagnosis_size nodes.\n",
        "                              if it finds, it marks it in the out_success list, which increases the chance of early termination.\n",
        "    pre_diagnosis_size      - the number of recent nodes to examine when pre_diagnosis is provided.\n",
        "    gates                   - the system gates list.\n",
        "    gate_rank_indexes       - list with the ranks of gates. see more details in the run_program method.\n",
        "    inputs_len              - the number of inputs.\n",
        "    outputs_len             - the number of outputs.\n",
        "    signals                 - the signals vector.\n",
        "    gates_input_indexes     - a list of lists. the i-th element holds the input indexes of gate i. \n",
        "    timeout_per_observation - a timeout in seconds for the observation run. \n",
        "    initial_time            - the time the dfs (root) began to run.\n",
        "\n",
        "    Returns:\n",
        "    True if it reaches timeout, or False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    global early_terminations\n",
        "\n",
        "    # in case a pre-diagnoses has been run, look in it for the new nodes in the path\n",
        "    if pre_diagnosis and random.random()<0.1:\n",
        "      \n",
        "      c = set(self.__path)\n",
        "      \n",
        "      for output_index, status in enumerate(self.__out_success):\n",
        "        if status == None:\n",
        "          for pd_item in pre_diagnosis[output_index]:\n",
        "            if pd_item <= c: # additional 'if', to eliminate the need for 2 checks\n",
        "              if pd_item < c:\n",
        "                self.__out_success[output_index] = 2\n",
        "              else:\n",
        "                self.__out_success[output_index] = 1\n",
        "\n",
        "\n",
        "    if all(self.__out_success):\n",
        "      early_terminations+=1\n",
        "      return False\n",
        "      \n",
        "\n",
        "    if self.__path: \n",
        "      # not root\n",
        "\n",
        "      # find the output vector, given the faulty gates\n",
        "      result = evaluate(gates, gate_rank_indexes, inputs_len, outputs_len, signals, faulty_gates=self.__path,  gates_input_indexes=gates_input_indexes)\n",
        "\n",
        "      # caluclate binary to decimal, to find the permutation index in the out_success array\n",
        "      result_int = sum([out_result * pow(2, (self.__n_outputs-1) - index) for index, out_result in enumerate(result)])\n",
        "\n",
        "      if self.__out_success[result_int] != 2:\n",
        "        # first time for this branch\n",
        "        self.__out_success[result_int] = 1\n",
        "\n",
        "      # when all the outputs have been diagnosed for this path, no need to continue the diagnosis\n",
        "      if all(self.__out_success):\n",
        "        early_terminations+=1\n",
        "        return False\n",
        "\n",
        "      # the last item in the path is the last in the full_path of all the gates\n",
        "      if self.__path[-1]==full_path[-1]:\n",
        "        return False\n",
        "      \n",
        "      next_index = full_path.index(self.__path[-1])+1\n",
        "        \n",
        "    else:\n",
        "      next_index = 0\n",
        " \n",
        "    # prune\n",
        "    if len(self.__path)==prune_level:\n",
        "      return False\n",
        "\n",
        "    # go deeper. begin with the next number after the last one in path\n",
        "    for i_element, element in enumerate(full_path[next_index:]):\n",
        "\n",
        "      self.__nodes.append(Node(self.__path + [element], self.__out_success, self.__n_gates, self.__n_outputs))\n",
        "\n",
        "      reached_timeout = self.__nodes[-1].dfs(full_path, prune_level, pre_diagnosis, pre_diagnosis_size, gates, gate_rank_indexes, inputs_len, outputs_len, signals, gates_input_indexes, timeout_per_observation, initial_time)\n",
        "      \n",
        "      if reached_timeout or (datetime.now() - initial_time).seconds >= timeout_per_observation: \n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def get_out_results(self, all_out, result):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns the results of the dfs run.\n",
        "\n",
        "    all_out - list with all the output permutations.\n",
        "    result  - container. received as empty.\n",
        "\n",
        "    Returns:\n",
        "    when the run finishes, the result variable will be a list of all the tree nodes. each item\n",
        "    will be a tuple of (<path>, <output>). \n",
        "    <path> indicates the path to the node (faulty gates).\n",
        "    <output> indicates the output signal.\n",
        "    \"\"\"\n",
        "\n",
        "    result.append((self.__path, [i[0] for i in zip(all_out, self.__out_success) if i[1]==1]))\n",
        "    for n in self.__nodes:\n",
        "      n.get_out_results(all_out, result)\n",
        "    \n",
        "  def print_me(self):\n",
        "    \"\"\"\n",
        "    prints the tree\n",
        "    \"\"\"\n",
        "    print(f\"path: {self.__path}, out_success: {self.__out_success}\")\n",
        "    for n in self.__nodes:\n",
        "      n.print_me()\n",
        "\n",
        "  @property\n",
        "  def nodes(self):\n",
        "    return self.__nodes\n",
        "\n",
        "  @property\n",
        "  def path(self):\n",
        "    return self.__path\n",
        "  \n",
        "  @property\n",
        "  def out_success(self):\n",
        "    return self.__out_success\n",
        "\n",
        "\n",
        "def tree_find_last_search(nd):\n",
        "  \"\"\"\n",
        "    returns the last path explored in the tree. \n",
        "    this function can be used to determine the search coverage in cases when timeout is being activated and the dfs stops in the middle of the run.\n",
        "  \"\"\"\n",
        "  while nd.nodes:\n",
        "    nd = nd.nodes[-1]\n",
        "  return nd.path"
      ],
      "metadata": {
        "id": "xw9wmm9tI6fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluations"
      ],
      "metadata": {
        "id": "nZjAIevZH4Sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate(gates, gate_rank_indexes, inputs_len, outputs_len, signals, faulty_gates, gates_input_indexes):\n",
        "\n",
        "    \"\"\"\n",
        "    returns the system's output, given the inputs.\n",
        "\n",
        "    gates               - list of the system gates.\n",
        "    gate_rank_indexes   - ist with the ranks of gates. see more details in the run_program method.\n",
        "    inputs_len          - the number of system inputs.\n",
        "    outputs_len         - the number of system outputs.\n",
        "    signals             - the system's signals vector.\n",
        "    faulty_gates        - a list of the faulty gates indexes.\n",
        "    gates_input_indexes - a list of lists. the i-th element holds the input indexes of gate i. \n",
        "\n",
        "    Returns:\n",
        "    a list with the system's output signals.\n",
        "    \"\"\"\n",
        "\n",
        "    # reset outputs and intermediate inputs. keep the inputs.\n",
        "    signals[inputs_len:] = [None]* (len(signals)-inputs_len)\n",
        "    \n",
        "    # initialize faulty gates\n",
        "    for gate in gates:\n",
        "      gate.faulty = False\n",
        "    for i in faulty_gates:\n",
        "      gates[i].faulty = True\n",
        "      \n",
        "    \n",
        "    # evaluation is done rank by rank, to get the intermediate z signal results\n",
        "    for gate_rank_indexes_i in gate_rank_indexes:\n",
        "      for item in gate_rank_indexes_i:\n",
        "\n",
        "        # load the input signals. To find their location in signals, use their indexes which are indicated in the gate.\n",
        "        in_signals = [signals[i] for i in gates_input_indexes[item]] \n",
        "       \n",
        "        # process the gate, and place its result on the out signals\n",
        "        signals[gates[item].output_index] = gates[item].process(in_signals) \n",
        "\n",
        "\n",
        "    return signals[inputs_len:inputs_len+outputs_len]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_diagnosis_probabilities(diagnosis, obs_out, p_noise, all_out, p_gate):\n",
        "  \n",
        "  \"\"\"\n",
        "  Returns the diagnosis probabilities for the given parameters.\n",
        "  \n",
        "  diagnosis - a list of list. each element i in the outer list is a list of diagnoses for the ouput permutation i.\n",
        "              (e.g: in a 4-outputs system diagnosis[2] will hold the diagnosis of the output obseravtion [0,0,1,0])\n",
        "  obs_out   - the observation output vector as given in the file.\n",
        "  p_noise   - a vector of probabilities of the noise per output.\n",
        "  all_out   - list with all the output permutations.\n",
        "  p_gate    - the probability of a gate to be faultuy.\n",
        "\n",
        "  Returns a list of probabilities per output permutation.\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  res = []\n",
        "\n",
        "  # because of the noise, we have to test each output as a possible output for the system.\n",
        "  # for each output, we will assume that it is the \"correct\" system output, and then that it was changed by the noise to become the \n",
        "  # observation output. For example: if the observation is [1,1], we will find for each output [[0,0],[0,1],[1,0],[1,1] the probability \n",
        "  # that the became [1,1] due to the noise ratios.\n",
        "  for out_index, out in enumerate(all_out):\n",
        "\n",
        "    # initialize with True or False by comparing to the observation\n",
        "    is_noise = [out[i]!=obs_out[i] for i in range(len(out))]\n",
        "    \n",
        "    # calculate probability to noise (or no noise) for each output\n",
        "    p_noise_per_output = [item[0] if item[1]==True else 1-item[0] for item in zip(p_noise,is_noise)]\n",
        "    \n",
        "    # mutiply the probabilities, to get the total out probablity (the probability of the \"out\" state to become obs_out, due to noise)\n",
        "    p_noise_total = 1\n",
        "    for i in p_noise_per_output:\n",
        "      p_noise_total = p_noise_total * i\n",
        "\n",
        "    # for each element in the diagnosis, calculate it's probablity to be faulty\n",
        "    p_d = []\n",
        "    for d in diagnosis[out_index]:\n",
        "      p_d.append (pow(p_gate, len(d)))\n",
        "\n",
        "    # normalize\n",
        "    sub_res = []\n",
        "    if sum(p_d) > 0:\n",
        "      factor = 1 / sum(p_d)\n",
        "      p_d = [i * factor for i in p_d]\n",
        "      \n",
        "      # the diagnosis probability equals to the probability of its gates to be faulty and the output to become the observation output\n",
        "      for item in p_d:\n",
        "        sub_res.append(item * p_noise_total)\n",
        "\n",
        "    res.append(sub_res)\n",
        "\n",
        "  return res\n",
        "\n",
        "\n",
        "def collect_diagnoses(root, all_out, pre_diagnosis, pre_diagnosis_level):\n",
        "  \n",
        "  \"\"\"\n",
        "  Scans the tree to collect the diagnoses.\n",
        "\n",
        "  root                  - Node. the dfs tree root.\n",
        "  all_out               - list with all the output permutations.\n",
        "  pre_diagnosis         - a list of diagnoses per output. \n",
        "                          when supplied, the method will merage it with the tree results.\n",
        "  pre_diagnosis_level   - the maximal size of diagnosis in the pre_diagnosis list.\n",
        "  \"\"\"\n",
        "\n",
        "  ret = []\n",
        "  candidates = []\n",
        "  full_sets = []\n",
        "\n",
        "  root.get_out_results(all_out, candidates)\n",
        "\n",
        "  for o in all_out:\n",
        "    full_sets.append(sorted([diagnosis for diagnosis, out in candidates if out==[o]], key=lambda x:len(x)))\n",
        "\n",
        "  if pre_diagnosis:\n",
        "\n",
        "    # convert pre_diagnosis from set to list\n",
        "    dg_list =[]\n",
        "    for item in pre_diagnosis:\n",
        "      dg_list.append([list(subitem) for subitem in item])\n",
        "    \n",
        "    for i in range(len(all_out)):\n",
        "\n",
        "      # remove the pre diagnosis from the full set (if exists)\n",
        "      small_size_diagnoses = [item for item in full_sets[i] if len(item)<=pre_diagnosis_level]\n",
        "      \n",
        "      for pr in pre_diagnosis:\n",
        "        if pr in small_size_diagnoses:\n",
        "            full_sets[i].remove(pr)\n",
        "      \n",
        "  # remove all super sets from within the outputs.\n",
        "  # on each full set, run per diagnosis size (size of set)\n",
        "  for full_set in full_sets:\n",
        "    \n",
        "    if full_set:\n",
        "\n",
        "      full_set = [set(i) for i in full_set]\n",
        "      new_full_set = []\n",
        "      items_lengths = [len(i) for i in full_set]\n",
        "      max_len = max(items_lengths)\n",
        "      \n",
        "      # size 1 has to run on 2 and above, then 2 has to run on 3 and above, etc..\n",
        "      for size in range(1,max_len):\n",
        "\n",
        "        current_size_items = [i for i in full_set if len(i)==size]\n",
        "        next_sizes_items = [index for index, item in enumerate(full_set) if len(item)>size]\n",
        "        \n",
        "        for x in current_size_items:\n",
        "          for y in next_sizes_items:\n",
        "            if x & full_set[y] == x:\n",
        "              full_set[y] = set()\n",
        "\n",
        "    ret.append([i for i in full_set if i])\n",
        "    \n",
        "    \n",
        "  return ret\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "3t6avrpUH9Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Print and Log Utilities"
      ],
      "metadata": {
        "id": "Ys9xCPgiGsun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def print_diagnoses(diagnoses, diagnoses_p, all_out):\n",
        "  \"\"\"\n",
        "  A utility method to return a formatted string of the given diagnosis.\n",
        "  \"\"\"\n",
        "\n",
        "  text = \"Diagnoses:\\n\"\n",
        "\n",
        "  for i in range(len(all_out)):\n",
        "    text += f\"\\nOutput {all_out[i]}:\\n\"\n",
        "\n",
        "    for i_d, d in enumerate(diagnoses[i]):\n",
        "      text += f\"{d}, P={diagnoses_p[i][i_d]}\\n\"  \n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "def print_results(final:bool, header:bool, to_screen:bool, to_file:bool, save_log:bool, system_name, gates, single_observation:bool, observations, \n",
        "                  observation_number, diagnoses, diagnoses_p, diagnoses_stats, p_noise, all_out, p_gate, duration, cache_used_counter, last_search_path, file_name_extension, pre_diagnosis_size):\n",
        "  \n",
        "  \"\"\"\n",
        "  A utility method to return a formatted string of the results.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  text = \"\"\n",
        "  file_name = f\"{DATA_FOLDER_PATH}_{system_name}_{file_name_extension}_Pre_{pre_diagnosis_size}_{f'diagnoses_{datetime.now().isoformat()}' if final else 'temp'}.txt\"\n",
        "  \n",
        "  if header:\n",
        "\n",
        "    file_mode = 'w'\n",
        "\n",
        "    if final:\n",
        "        \n",
        "      sec = duration.total_seconds()\n",
        "      time_text = \"Run completed in \"\n",
        "      if sec >= 60:\n",
        "        time_text += f\"{sec//60} minute\" + (\"s\" if sec>=120 else \"\") + \" and \"\n",
        "      time_text += str(round((sec - (sec//60) * 60),1)) + \" seconds\"\n",
        "\n",
        "    else:\n",
        "      time_text =\"\"\n",
        "      \n",
        "      \n",
        "    # title with system properties\n",
        "    title = f\"DIAGNOSIS RESULTS FOR SYSTEM {system_name.upper()}\"\n",
        "    text = f\"\\n{'#'*len(title)}\\n{title}\\n{'#'*len(title)}\\n\\n\"\n",
        "    if time_text:\n",
        "      text+=f\"{time_text}\\n\\n\"\n",
        "    text += \"Gate indexes:\\n\"\n",
        "    for i,gate in enumerate(gates):\n",
        "      text += f\"{i} = {gate.name}\\n\"\n",
        "\n",
        "    text += f\"\\nGate probability to be faulty:\\n{p_gate}\\n\"\n",
        "\n",
        "    text += \"\\nNoise probability:\\n\"\n",
        "    for i,p in enumerate(p_noise):\n",
        "      text += f\"Out{i} = {p}\\n\"\n",
        "\n",
        "    text += f\"\\nPre Diagnosis Size: {pre_diagnosis_size}\\n\"\n",
        "\n",
        "  else:\n",
        "    file_mode = 'a'\n",
        "\n",
        "  if single_observation:\n",
        "        \n",
        "    \n",
        "    sub_title = f\"Observation #{observation_number+1} - {observations}:\"\n",
        "    text += f\"\\n\\n{sub_title}\\n{'-'*len(sub_title)}\\n\\n\"\n",
        "\n",
        "    sec = duration.total_seconds()\n",
        "    time_text = \"Observation run completed in \"\n",
        "    if sec >= 60:\n",
        "      time_text += f\"{sec//60} minute\" + (\"s\" if sec>=120 else \"\") + \" and \"\n",
        "    time_text += str(round((sec - (sec//60) * 60),1)) + \" seconds\"\n",
        "\n",
        "    text += time_text + \"\\n\\n\"\n",
        "\n",
        "    text += print_diagnoses(diagnoses, diagnoses_p, all_out)\n",
        "\n",
        "  else:\n",
        "\n",
        "    if observations:\n",
        "      \n",
        "      average_early_terminations = np.average([diagnoses_stats[i][3] for i in diagnoses_stats.keys()])\n",
        "\n",
        "      # number of observations and cache usage\n",
        "      text += f\"\\nNumber of observations: {len(observations)}\\n\"\n",
        "      text += f\"\\nAverage Early Terminations {average_early_terminations:.2f}\\n\"\n",
        "      text += f\"\\nResults from cache have been used {cache_used_counter} times\\n\"\n",
        "      text += \"\\n\"\n",
        "\n",
        "      # general stats per observation and timeouts\n",
        "      text += f\"Timeouts\\n--------\\n\\n\"\n",
        "      text += f\"Timeout per Observation: {TIMEOUT_PER_OBSERVATION} seconds\\n\"\n",
        "      reached_timeout_count = len([i for i in diagnoses_stats.keys() if diagnoses_stats[i][0]])\n",
        "      average_total_diagnoses = np.average([diagnoses_stats[i][1] for i in diagnoses_stats.keys()])\n",
        "      average_diagnoses_per_output = average_total_diagnoses / len(all_out) #np.average([diagnoses_stats[i][2] for i in diagnoses_stats.keys()])\n",
        "      average_diagnoses_size = np.average([len(subitem) for sublist in diagnoses for item in sublist for subitem in item])\n",
        "      \n",
        "      text += f\"Reached Timeout: {reached_timeout_count}/{len(diagnoses_stats.keys())} \\\n",
        "                \\nAverage Diagnoses per Observation: {average_total_diagnoses:.2f} \\\n",
        "                \\nAverage Diagnosis Size: {average_diagnoses_size:.2f}        \\\n",
        "                \\nAverage Diagnoses per Output: {average_diagnoses_per_output:.2f}\\n\\n\"\n",
        "\n",
        "      text += f\"Observation #  -  Reached Timeout  -  Total Diagnoses  -  Average Diagnoses per Output  - Average Diagnosis Size\\n\"\n",
        "      for i in diagnoses_stats.keys():\n",
        "        text += f\"{(i+1):4.0f}\\t-\\t{diagnoses_stats[i][0]}\\t-\\t{diagnoses_stats[i][1]}\\t-\\t{diagnoses_stats[i][2]:.1f}\\t-\\t{np.average([len(item) for sublist in diagnoses[i] for item in sublist]):.2f}\\n\"\n",
        "    \n",
        "      for i, observation in enumerate(observations):\n",
        "        sub_title = f\"Observation #{i+1} - {observation}:\"\n",
        "        text += f\"\\n\\n{sub_title}\\n{'-'*len(sub_title)}\\n\\n\"\n",
        "        text += print_diagnoses(diagnoses[i], diagnoses_p[i], all_out)\n",
        "\n",
        "\n",
        "      text += f\"\\nLast search path:\\n{last_search_path}\"\n",
        "\n",
        "  if to_screen:  \n",
        "    print(text)\n",
        "  \n",
        "  if to_file:\n",
        "\n",
        "    with open(file_name, file_mode) as f:\n",
        "      f.write(text)\n",
        "  \n",
        "  if save_log:\n",
        "    file_name = system_name + \"_diagnoses_log_\" + datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\") + \".txt\"\n",
        "    with open(file_name, 'w') as f:\n",
        "      for item in db:\n",
        "        f.write(str(item))\n",
        "        f.write(\"\\n\")\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "s-i_eo_fG3BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "rC2OjodEv6dI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR3LEcK2oI4t"
      },
      "outputs": [],
      "source": [
        "run_timer   = datetime.now()\n",
        "run_counter = 0\n",
        "system      = '74182_short' # '74283_short', 'c17', '74182_short', 'c432_short', 'c3540_short'\n",
        "\n",
        "\n",
        "run_program(system, 'RANKED', pre_diagnosis_level=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_program(system, gates_queue_method, pre_diagnosis_level):\n",
        "  \"\"\"\n",
        "  Runs the program\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  system:                  str,\n",
        "                           name of the iscas-85 system \n",
        "  gates_queue_method:      str,\n",
        "                           the default method in which the gates will be ordered in the DFS tree. \n",
        "                           'SERIAL' - according to the order in the ISCAS-85 file. Left most node is 0.\n",
        "                           'RANKED' - considers the model gate ranks. The higher the proximity to the outputs, the higher the rank. High ranked gates are accessed first. \n",
        "  pre_diagnosis_level:     int, \n",
        "                           the depth of the pre-diagnosis which is used to construct a the pre-diagnosis tree. see documentation for more details.\n",
        "  \"\"\"\n",
        "\n",
        "  global early_terminations\n",
        "  start_time = datetime.now()\n",
        "    \n",
        "  gates=[]\n",
        "  signals = []\n",
        "  all_out = []\n",
        "  diagnoses = []\n",
        "  diagnoses_p = []\n",
        "  gate_rank_indexes = []\n",
        "  p_gate = 0.01 # the probability of a gate to be faulty\n",
        "\n",
        "  print(\"initializing gates...\")\n",
        "  gates, signals, inputs_num, outputs_num = initialize_gates(system, p_gate)\n",
        "  \n",
        "  # gates_input_indexes is a list of lists. the i-th element holds the input indexes of gate i\n",
        "  gates_input_indexes = [g.input_indexes for g in gates]\n",
        "\n",
        "  # gate_rank_indexes is a list of lists. the i-th element holds the gate numbers of rank i\n",
        "  print(\"initializing ranks...\")\n",
        "  gate_rank_indexes, max_rank = initialize_ranks(gates)\n",
        "  \n",
        "  print(\"initializing observations...\")\n",
        "  observations = initialize_observations(system, gates, len(signals), inputs_num, outputs_num)\n",
        "  \n",
        "  print(\"initializing noise probabilities...\")\n",
        "  p_noise = initialize_noise(outputs_num, print_result=True)\n",
        "\n",
        "  print(\"initializing all output possibilities...\")\n",
        "  all_out = initialize_permutations(outputs_num)\n",
        "\n",
        "  print_results(final=False, header=True, to_screen=False, to_file=True, save_log=False, system_name=system, gates=gates, single_observation=False, observations=None, \n",
        "                observation_number=None, diagnoses=None, diagnoses_p=None, diagnoses_stats=None, p_noise=p_noise, all_out=all_out, p_gate=p_gate, duration=None, \n",
        "                cache_used_counter=None, last_search_path=None, file_name_extension=gates_queue_method, pre_diagnosis_size=pre_diagnosis_level)\n",
        "\n",
        "  # find the diagnoses and their probabilities\n",
        "  print(f\"Diagnosing {len(observations)} observations using DFS (timeout per observation = {TIMEOUT_PER_OBSERVATION} [sec])...\\n\")\n",
        "\n",
        "  diagnoses_cache = {}\n",
        "  diagnoses_stats = {}\n",
        "  cache_used_counter = 0\n",
        "\n",
        "  for observation_i, observation in enumerate(observations):\n",
        "\n",
        "    start_time_temp = datetime.now()\n",
        "\n",
        "    print(f\"\\nObservation {observation_i},\", end=\"\")\n",
        "    print(observation[:inputs_num])\n",
        "    inputs_str = \"\".join([str(i) for i in observation[:inputs_num]])\n",
        "    current_diagnosis = None\n",
        "\n",
        "    # check existence in cache\n",
        "    if inputs_str in diagnoses_cache.keys():\n",
        "      \n",
        "      current_diagnosis = diagnoses_cache[inputs_str]\n",
        "      print(f\"{observation_i} taken from cache!\")\n",
        "      cache_used_counter += 1\n",
        "\n",
        "    else:\n",
        "      \n",
        "      # insert inputs\n",
        "      signals = [None] * len(signals)\n",
        "      obs_in = observation[:inputs_num]\n",
        "      for i in range(inputs_num):\n",
        "        signals[i] = obs_in[i]\n",
        "\n",
        "      \n",
        "      full_path = get_gates_sequence(gates_queue_method, gates, gate_rank_indexes)\n",
        "  \n",
        "      pre_diagnosis_size = pre_diagnosis_level\n",
        "      pre_diagnosis = []\n",
        "\n",
        "      if pre_diagnosis_level > 0:\n",
        "        print(f\"Running pre-diagnosis {pre_diagnosis_level}-size)...\")\n",
        "\n",
        "        root = Node(path=[], branch_out_success=[None] * pow(2, outputs_num), n_gates=len(gates), n_outputs=outputs_num)\n",
        "        root.dfs(full_path, pre_diagnosis_level, [], 0, gates, gate_rank_indexes, inputs_num, outputs_num, signals, gates_input_indexes, TIMEOUT_PER_OBSERVATION, datetime.now())\n",
        "        \n",
        "        pre_diagnosis = collect_diagnoses(root, all_out, [], 0)\n",
        "        print(pre_diagnosis)\n",
        "\n",
        "      # initialize tree\n",
        "      root = Node(path=[], branch_out_success=[None] * pow(2, outputs_num), n_gates=len(gates), n_outputs=outputs_num)\n",
        "      early_terminations = 0\n",
        "      reached_timeout = root.dfs(full_path, -1, pre_diagnosis, pre_diagnosis_size, gates, gate_rank_indexes, inputs_num, outputs_num, signals, gates_input_indexes, TIMEOUT_PER_OBSERVATION, datetime.now())\n",
        "      print(\"early_terminations:\", early_terminations)\n",
        "\n",
        "      current_diagnosis = collect_diagnoses(root, all_out, pre_diagnosis, pre_diagnosis_level)\n",
        "      \n",
        "      current_diagnosis_count = [len(l) for l in current_diagnosis]\n",
        "      diagnoses_stats[observation_i] = [reached_timeout, sum(current_diagnosis_count),np.average(current_diagnosis_count), early_terminations]\n",
        "\n",
        "      # store in cache\n",
        "      diagnoses_cache[inputs_str] = current_diagnosis\n",
        "      \n",
        "    p = calculate_diagnosis_probabilities(current_diagnosis, observation[inputs_num:], p_noise, all_out, p_gate)\n",
        "    diagnoses.append(current_diagnosis)\n",
        "    diagnoses_p.append(p)\n",
        "\n",
        "    print_results(final=False, header=False, to_screen=False, to_file=True, save_log=False, system_name=system, gates=gates, single_observation=True, observations=observation, observation_number=observation_i, \n",
        "                  diagnoses=current_diagnosis, diagnoses_p=p, diagnoses_stats=None, p_noise=p_noise, all_out=all_out, p_gate=p_gate, duration=datetime.now()-start_time_temp, \n",
        "                  cache_used_counter=None, last_search_path=None, file_name_extension=gates_queue_method, pre_diagnosis_size=pre_diagnosis_size)\n",
        "\n",
        "       \n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"Preparing results...\")\n",
        "  print_results(final=True, header=True, to_screen=False, to_file=True, save_log=False, system_name=system, gates=gates, single_observation=False, observations=observations,\n",
        "                observation_number=None, diagnoses=diagnoses, diagnoses_p=diagnoses_p, diagnoses_stats=diagnoses_stats, p_noise=p_noise, all_out=all_out, p_gate=p_gate, duration=datetime.now()-start_time, \n",
        "                cache_used_counter=cache_used_counter, last_search_path = tree_find_last_search(root), file_name_extension=gates_queue_method, pre_diagnosis_size=pre_diagnosis_size)\n",
        "\n",
        "  print(\"Done.\")\n"
      ],
      "metadata": {
        "id": "HyDq7OMzwB6n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}